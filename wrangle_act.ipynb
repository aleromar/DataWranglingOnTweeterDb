{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling on WeRateDogs\n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Gather](#gather)\n",
    "- [Assess](#assess)\n",
    "- [Clean](#clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "The main purpose of this project is to put into practice key concepts learnt during this module. Tasks that will be carrier out on the following notebook are gathering, assessing and cleaning data that has been provided to us using different means. In order to do so Python 3 will be used. Also the solution provided will lean on the following set of python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import tweepy\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "## Gather    \n",
    "\n",
    "The first step in data analysis is data gathering. In this case data gathering will be comprised of the following steps:   \n",
    "**1.-** Import the tweeter archive for WeRateDogs into the workspace   \n",
    "**2.-** Download image predictions data from a given URL   \n",
    "**3.-** Gather data from the Tweeter API to collect more relevant information   \n",
    "\n",
    "#### 1.- Import the tweeter archive\n",
    "This file has already been provided as a manual download. As such, it's available locally and just need to import it using pandas read_csv method for reading flat files into a pandas dataframe. Then, output the first row to verify that it has loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>retweeted_status_user_id</th>\n",
       "      <th>retweeted_status_timestamp</th>\n",
       "      <th>expanded_urls</th>\n",
       "      <th>rating_numerator</th>\n",
       "      <th>rating_denominator</th>\n",
       "      <th>name</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892420643555336193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-01 16:23:56 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Phineas. He's a mystical boy. Only eve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/892420643...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Phineas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  in_reply_to_status_id  in_reply_to_user_id  \\\n",
       "0  892420643555336193                    NaN                  NaN   \n",
       "\n",
       "                   timestamp  \\\n",
       "0  2017-08-01 16:23:56 +0000   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                                text  retweeted_status_id  \\\n",
       "0  This is Phineas. He's a mystical boy. Only eve...                  NaN   \n",
       "\n",
       "   retweeted_status_user_id retweeted_status_timestamp  \\\n",
       "0                       NaN                        NaN   \n",
       "\n",
       "                                       expanded_urls  rating_numerator  \\\n",
       "0  https://twitter.com/dog_rates/status/892420643...                13   \n",
       "\n",
       "   rating_denominator     name doggo floofer pupper puppo  \n",
       "0                  10  Phineas  None    None   None  None  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_df = pd.read_csv(\"twitter_archive_enhanced.csv\")\n",
    "archive_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.- Download image predictions data from a given URL   \n",
    "Unlike the previous file, in this case the file's URL was given. Hence, we download the file programatically as this is less prone to errors and eases reproducibility.    \n",
    "First download the file using python library **_requests_** and save the information downloaded in a file. Then, import the file similarly to how it was done in step \\#1. Also output first row to verify everything has gone correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1_conf</th>\n",
       "      <th>p1_dog</th>\n",
       "      <th>p2</th>\n",
       "      <th>p2_conf</th>\n",
       "      <th>p2_dog</th>\n",
       "      <th>p3</th>\n",
       "      <th>p3_conf</th>\n",
       "      <th>p3_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>666020888022790149</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Welsh_springer_spaniel</td>\n",
       "      <td>0.465074</td>\n",
       "      <td>True</td>\n",
       "      <td>collie</td>\n",
       "      <td>0.156665</td>\n",
       "      <td>True</td>\n",
       "      <td>Shetland_sheepdog</td>\n",
       "      <td>0.061428</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                          jpg_url  \\\n",
       "0  666020888022790149  https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg   \n",
       "\n",
       "   img_num                      p1   p1_conf  p1_dog      p2   p2_conf  \\\n",
       "0        1  Welsh_springer_spaniel  0.465074    True  collie  0.156665   \n",
       "\n",
       "   p2_dog                 p3   p3_conf  p3_dog  \n",
       "0    True  Shetland_sheepdog  0.061428    True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "image_predictions = requests.get(url)\n",
    "file_name = url.split('/')[-1]\n",
    "with open(file_name, mode='w', encoding =image_predictions.encoding) as f:\n",
    "    f.write(image_predictions.text)\n",
    "image_df = pd.read_csv(file_name,sep='\\t')\n",
    "image_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.- Gather data from Tweeter by accessing their API \n",
    "The third step for this data gathering effort is to obtain various pieces of data from Tweeter directly by using their API. Please note that keys and tokens are stored in a separate file that is stored locally and is not part of source control. As such, these fields will need to be edited if this notebook was to used different tweeter app credentials.    \n",
    "The idea is to query Tweeter's API through python's api library for tweeter (Tweepy). The idea is to get some more information for each of the tweets we have in df_archive dataframe. Then store json content into a txt file. Once the json text file is available parse it and create a new dataframe to work under python environment. \n",
    "So let's first define the function that will be used to parse json text file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseJson():\n",
    "    jsonlist = []\n",
    "    with open('tweet_json.txt','r',encoding='utf-8') as fjsonRead:\n",
    "        for line in fjsonRead:\n",
    "            json_dict = json.loads(line)\n",
    "            tweet_id = json_dict['id']\n",
    "            retw_count = json_dict['retweet_count']\n",
    "            fav_count = json_dict['favorite_count']\n",
    "            jsonlist.append({'tweet_id':tweet_id, 'retw_count':retw_count, 'fav_count':fav_count})\n",
    "    return pd.DataFrame(jsonlist,columns=['tweet_id', 'retw_count', 'fav_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading information from the API for so many tweets it's a time consuming task. Thus, we would not like to do it every time this notebook is run since this operation can take 30 minutes on its own. Therefore if the file is present, it's assumed that the API has already been queried and data stored in the file. If this was the case, then proceed with parsing the file into a dataframe. Otherwise, query the API and store data in a text file before parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>retw_count</th>\n",
       "      <th>fav_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892420643555336193</td>\n",
       "      <td>8366</td>\n",
       "      <td>38195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  retw_count  fav_count\n",
       "0  892420643555336193        8366      38195"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    tweeter_df = parseJson()\n",
    "except FileNotFoundError as e:\n",
    "    start = timer()\n",
    "    with open('tweet_json.txt','w',encoding='utf-8') as f:\n",
    "        with open('tweetIdsFailed.txt','w',encoding='utf-8') as ffail:\n",
    "            with open('tweeterKeys.txt','r',encoding='utf-8') as f:\n",
    "                getData = lambda line: line.split(' ')[0]\n",
    "                consumer_key = getData(f.readline())\n",
    "                consumer_secret = getData(f.readline())\n",
    "                access_token = getData(f.readline())\n",
    "                access_secret = getData(f.readline())\n",
    "                auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "                auth.set_access_token(access_token, access_secret)\n",
    "                api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "                for row in archive_df['tweet_id']:\n",
    "                    try:\n",
    "                        tweet = api.get_status(row,tweet_mode='extended')\n",
    "                        json.dump(tweet._json, f)\n",
    "                        f.write('\\n')\n",
    "                    except tweepy.TweepError as e:\n",
    "                        ffail.write(str(row)+'\\n')\n",
    "    end = timer()\n",
    "    print('Time: {}'.format(end-start))\n",
    "    tweeter_df = parseJson()\n",
    "tweeter_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "## Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the required information has been collected via Tweeter API, then we need to join it with data in archive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df = archive_df.merge(df_tweeter,on='tweet_id',how='left')\n",
    "archive_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
