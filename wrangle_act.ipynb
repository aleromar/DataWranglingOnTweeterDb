{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling on WeRateDogs\n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Gather](#gather)\n",
    "- [Assess](#assess)\n",
    "- [Clean](#clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "The main purpose of this project is to put into practice key concepts learnt during this module. Tasks that will be carrier out on the following notebook are gathering, assessing and cleaning data that has been provided to us using different means. In order to do so Python 3 will be used. Also the solution provided will lean on the following set of python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import tweepy\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "## Gather    \n",
    "\n",
    "The first step in data analysis is data gathering. In this case data gathering will be comprised of the following steps:   \n",
    "**1.-** Import the tweeter archive for WeRateDogs into the workspace   \n",
    "**2.-** Download image predictions data from a given URL   \n",
    "**3.-** Gather data from the Tweeter API to collect more relevant information   \n",
    "\n",
    "#### 1.- Import the tweeter archive\n",
    "This file has already been provided as a manual download. As such, it's available locally and just need to import it using pandas read_csv method for reading flat files into a pandas dataframe. Then, output the first row to verify that it has loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>retweeted_status_user_id</th>\n",
       "      <th>retweeted_status_timestamp</th>\n",
       "      <th>expanded_urls</th>\n",
       "      <th>rating_numerator</th>\n",
       "      <th>rating_denominator</th>\n",
       "      <th>name</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892420643555336193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-01 16:23:56 +0000</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>This is Phineas. He's a mystical boy. Only eve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/dog_rates/status/892420643...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>Phineas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  in_reply_to_status_id  in_reply_to_user_id  \\\n",
       "0  892420643555336193                    NaN                  NaN   \n",
       "\n",
       "                   timestamp  \\\n",
       "0  2017-08-01 16:23:56 +0000   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "\n",
       "                                                text  retweeted_status_id  \\\n",
       "0  This is Phineas. He's a mystical boy. Only eve...                  NaN   \n",
       "\n",
       "   retweeted_status_user_id retweeted_status_timestamp  \\\n",
       "0                       NaN                        NaN   \n",
       "\n",
       "                                       expanded_urls  rating_numerator  \\\n",
       "0  https://twitter.com/dog_rates/status/892420643...                13   \n",
       "\n",
       "   rating_denominator     name doggo floofer pupper puppo  \n",
       "0                  10  Phineas  None    None   None  None  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_df = pd.read_csv(\"twitter_archive_enhanced.csv\")\n",
    "archive_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.- Download image predictions data from a given URL   \n",
    "Unlike the previous file, in this case the file's URL was given. Hence, we download the file programatically as this is less prone to errors and eases reproducibility.    \n",
    "First download the file using python library **_requests_** and save the information downloaded in a file. Then, import the file similarly to how it was done in step \\#1. Also output first row to verify everything has gone correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1_conf</th>\n",
       "      <th>p1_dog</th>\n",
       "      <th>p2</th>\n",
       "      <th>p2_conf</th>\n",
       "      <th>p2_dog</th>\n",
       "      <th>p3</th>\n",
       "      <th>p3_conf</th>\n",
       "      <th>p3_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>666020888022790149</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Welsh_springer_spaniel</td>\n",
       "      <td>0.465074</td>\n",
       "      <td>True</td>\n",
       "      <td>collie</td>\n",
       "      <td>0.156665</td>\n",
       "      <td>True</td>\n",
       "      <td>Shetland_sheepdog</td>\n",
       "      <td>0.061428</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                          jpg_url  \\\n",
       "0  666020888022790149  https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg   \n",
       "\n",
       "   img_num                      p1   p1_conf  p1_dog      p2   p2_conf  \\\n",
       "0        1  Welsh_springer_spaniel  0.465074    True  collie  0.156665   \n",
       "\n",
       "   p2_dog                 p3   p3_conf  p3_dog  \n",
       "0    True  Shetland_sheepdog  0.061428    True  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "image_predictions = requests.get(url)\n",
    "file_name = url.split('/')[-1]\n",
    "with open(file_name, mode='w', encoding =image_predictions.encoding) as f:\n",
    "    f.write(image_predictions.text)\n",
    "image_df = pd.read_csv(file_name,sep='\\t')\n",
    "image_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.- Gather data from Tweeter by accessing their API \n",
    "The third step for this data gathering effort is to obtain various pieces of data from Tweeter directly by using their API. Please note that keys and tokens are stored in a separate file that is stored locally and is not part of source control. As such, these fields will need to be edited if this notebook was to used different tweeter app credentials.    \n",
    "The idea is to query Tweeter's API through python's api library for tweeter (Tweepy). The idea is to get some more information for each of the tweets we have in df_archive dataframe. Then store json content into a txt file. Once the json text file is available parse it and create a new dataframe to work under python environment. \n",
    "So let's first define the function that will be used to parse json text file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseJson():\n",
    "    jsonlist = []\n",
    "    with open('tweet_json.txt','r',encoding='utf-8') as fjsonRead:\n",
    "        for line in fjsonRead:\n",
    "            json_dict = json.loads(line)\n",
    "            tweet_id = json_dict['id']\n",
    "            retw_count = json_dict['retweet_count']\n",
    "            fav_count = json_dict['favorite_count']\n",
    "            jsonlist.append({'tweet_id':tweet_id, 'retw_count':retw_count, 'fav_count':fav_count})\n",
    "    return pd.DataFrame(jsonlist,columns=['tweet_id', 'retw_count', 'fav_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading information from the API for so many tweets it's a time consuming task. Thus, we would not like to do it every time this notebook is run since this operation can take 30 minutes on its own. Therefore if the file is present, it's assumed that the API has already been queried and data stored in the file. If this was the case, then proceed with parsing the file into a dataframe. Otherwise, query the API and store data in a text file before parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>retw_count</th>\n",
       "      <th>fav_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892420643555336193</td>\n",
       "      <td>8366</td>\n",
       "      <td>38195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  retw_count  fav_count\n",
       "0  892420643555336193        8366      38195"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    tweeter_df = parseJson()\n",
    "except FileNotFoundError as e:\n",
    "    start = timer()\n",
    "    with open('tweet_json.txt','w',encoding='utf-8') as f:\n",
    "        with open('tweetIdsFailed.txt','w',encoding='utf-8') as ffail:\n",
    "            with open('tweeterKeys.txt','r',encoding='utf-8') as f:\n",
    "                getData = lambda line: line.split(' ')[0]\n",
    "                consumer_key = getData(f.readline())\n",
    "                consumer_secret = getData(f.readline())\n",
    "                access_token = getData(f.readline())\n",
    "                access_secret = getData(f.readline())\n",
    "                auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "                auth.set_access_token(access_token, access_secret)\n",
    "                api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "                for row in archive_df['tweet_id']:\n",
    "                    try:\n",
    "                        tweet = api.get_status(row,tweet_mode='extended')\n",
    "                        json.dump(tweet._json, f)\n",
    "                        f.write('\\n')\n",
    "                    except tweepy.TweepError as e:\n",
    "                        ffail.write(str(row)+'\\n')\n",
    "    end = timer()\n",
    "    print('Time: {}'.format(end-start))\n",
    "    tweeter_df = parseJson()\n",
    "tweeter_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all data has been collected, we need to inspect each dataframe to find flaws in either contents or structure. Let's start with archive_df. Let's first have a general look by printing the info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2356 entries, 0 to 2355\n",
      "Data columns (total 17 columns):\n",
      "tweet_id                      2356 non-null int64\n",
      "in_reply_to_status_id         78 non-null float64\n",
      "in_reply_to_user_id           78 non-null float64\n",
      "timestamp                     2356 non-null object\n",
      "source                        2356 non-null object\n",
      "text                          2356 non-null object\n",
      "retweeted_status_id           181 non-null float64\n",
      "retweeted_status_user_id      181 non-null float64\n",
      "retweeted_status_timestamp    181 non-null object\n",
      "expanded_urls                 2297 non-null object\n",
      "rating_numerator              2356 non-null int64\n",
      "rating_denominator            2356 non-null int64\n",
      "name                          2356 non-null object\n",
      "doggo                         2356 non-null object\n",
      "floofer                       2356 non-null object\n",
      "pupper                        2356 non-null object\n",
      "puppo                         2356 non-null object\n",
      "dtypes: float64(4), int64(3), object(10)\n",
      "memory usage: 313.0+ KB\n"
     ]
    }
   ],
   "source": [
    "archive_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information above is filtered data from the 5000+ Tweeter account. It's filtered taking into account only those tweets that contain dog ratings. However as it can be seen above some of the tweets are **re**tweets and so we can regard these as duplicated information. We could verify that **retweeted_status_id** == tweet_id for other row in the dataframe. The problem here is that the type for column retweeted_status_id is float64. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.874739571039519e+17 887473957103951872\n"
     ]
    }
   ],
   "source": [
    "print (archive_df[archive_df['retweeted_status_id'].notnull()].retweeted_status_id.values[0],\n",
    "       archive_df[archive_df['retweeted_status_id'].notnull()].retweeted_status_id.astype('int64').values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To me the above results are wrong. The number on the left, in floating point arithmetic, contains only 15 decimal digits. The exponent however indicates 17. Therefore when casting to an **int64** type, somehow the last two digits are made up...\n",
    "# ASK ABOUT THIS BEHAVIOR\n",
    "Therefore and unless this data is gathered it's impossible to verify whether all retweets are already contained in the archive_df in the form of normal tweets. At this point there are two options. \n",
    "> First, search for this missing information in the downloaded json file\n",
    "> Or second, assumme that all retweets' information is already present in archive_df in the form of normal tweets\n",
    "\n",
    "Let's take the first option. This also demonstrates the spirit of iteration for which there was much emphasis in the class. Let's first create a new column called **retweeted_status_id_2** that will contain the integer retweeted_id. This column will be in string format and so no information will be lost. Then, we will compare both retweeted_status_id with retweeted_status_id2 to verify that they are not the same. Let's _gather_ the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweet id for which the tweet was a retweet\n",
    "retw = archive_df[archive_df['retweeted_status_id'].notnull()].tweet_id.values\n",
    "archive_df['retweeted_status_id_2'] = np.NaN\n",
    "with open('tweet_json.txt','r',encoding='utf-8') as fjsonRead:\n",
    "    for line in fjsonRead:\n",
    "        json_dict = json.loads(line)\n",
    "        if json_dict['id'] in retw:\n",
    "            archive_df.loc[archive_df[archive_df['tweet_id']==json_dict['id']].index,'retweeted_status_id_2'] = str(json_dict['retweeted_status']['id'])\n",
    "        else:\n",
    "            archive_df.loc[archive_df[archive_df['tweet_id']==json_dict['id']].index,'retweeted_status_id_2'] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the information has been collected, let's compare both columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0, -1, -3,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "       -2,  0,  0,  0,  0,  0, -2, -1,  0, -1,  0, -2, -1, -1, -2,  0,  0,\n",
       "        0, -2, -3, -2,  0, -2,  0,  0, -1, -1,  0,  0, -5, -1, -2, -1,  0,\n",
       "        0,  0,  0,  0, -4, -5,  0,  0,  0,  0,  0,  0,  0, -2,  0,  0, -4,\n",
       "        0,  0,  0, -2,  0,  0, -1, -2,  0,  0,  0,  0,  0,  0,  0,  0, -2,\n",
       "        0, -1,  0,  0,  0,  0,  0, -5,  0,  0,  0,  0,  0,  0, -4,  0,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0, -1,  0,  0, -5,  0,\n",
       "        0, -1,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,\n",
       "       -1,  0, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0, -2,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0, -1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retw = archive_df[archive_df['retweeted_status_id_2'].notnull()]\n",
    "(retw.retweeted_status_id.astype('int64') - retw.retweeted_status_id_2.astype('int64')).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results above. They indicate that the casting from float to int64 does not yield correct results and so it would have been wrong to analyse using a casting method. Therefore the information needs to be extracted from the API itself. Now that we have the right information let's see how many of these retweets are already present in the whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.isin(retw.retweeted_status_id_2.astype('int64').values,archive_df.tweet_id.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen by the number 145 above that some entries in the archive_df are duplicates of original tweets.    \n",
    "> **Some retweets contain duplicate information**     \n",
    "\n",
    "Also mentioned above, some column types are wrong. Columns that are id(s) should not be float64. Also timestamp columns should be casted to datetime\n",
    "> **Some column types are wrong**\n",
    "\n",
    "In a similar way, when this twitter account replied to some tweets it's possible that both original and reply contain a score. In this case the second one overrides the first and so we should only consider the last score to be the valid one. Let's proceed as we did for the retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original message:\n",
      "    This is Pipsy. He is a fluffball. Enjoys traveling the sea &amp; getting tangled in leash. 12/10 I would kill for Pipsy https://t.co/h9R0EwKd9X\n",
      "In reply to the previous\n",
      "    Ladies and gentlemen... I found Pipsy. He may have changed his name to Pablo, but he never changed his love for the sea. Pupgraded to 14/10 https://t.co/lVU5GyNFen\n",
      "Total number of replies for which the original is in archive_df : 44\n"
     ]
    }
   ],
   "source": [
    "# Get tweet id for which the tweet was a retweet\n",
    "retw = archive_df[archive_df['in_reply_to_status_id'].notnull()].tweet_id.values\n",
    "archive_df['in_reply_to_status_id_2'] = np.NaN\n",
    "with open('tweet_json.txt','r',encoding='utf-8') as fjsonRead:\n",
    "    for line in fjsonRead:\n",
    "        json_dict = json.loads(line)\n",
    "        if json_dict['id'] in retw:\n",
    "            if str(json_dict['in_reply_to_status_id']) == 'None':\n",
    "                archive_df.loc[archive_df[archive_df['tweet_id']==json_dict['id']].index,'in_reply_to_status_id'] = np.NaN\n",
    "                archive_df.loc[archive_df[archive_df['tweet_id']==json_dict['id']].index,'in_reply_to_user_id'] = np.NaN\n",
    "            else:\n",
    "                archive_df.loc[archive_df[archive_df['tweet_id']==json_dict['id']].index,'in_reply_to_status_id_2'] = str(json_dict['in_reply_to_status_id'])\n",
    "        else:\n",
    "            archive_df.loc[archive_df[archive_df['tweet_id']==json_dict['id']].index,'in_reply_to_status_id_2'] = np.NaN  \n",
    "retw = archive_df[archive_df['in_reply_to_status_id_2'].notnull()]\n",
    "print('Original message:')\n",
    "print('    {}'.format( archive_df.loc[archive_df.tweet_id == np.int64(retw.iloc[5].in_reply_to_status_id_2)].text.values[0]))\n",
    "print('In reply to the previous')\n",
    "print('    {}'.format(retw.iloc[5].text))\n",
    "numberreplies = sum(np.isin(retw.in_reply_to_status_id_2.astype('int64').values,archive_df.tweet_id.values))\n",
    "print('Total number of replies for which the original is in archive_df : {}'.format(numberreplies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the above that there are 44 replies that need to be removed\n",
    "> **Some replies contain duplicated score**\n",
    "\n",
    "Coming back to archive_df.info(), it can be observed that some of the values in expanded_url are missing. This information needs to be gathered\n",
    "> **Expanded_url column contains missing information**\n",
    "\n",
    "Let's now check what happens with dog names. I guess that a regex statement has been used to extract dog names on this column. It's possible that the standard regex may extract wrong information, so let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['None', 'a', 'Charlie', 'Cooper', 'Oliver', 'Lucy', 'Penny', 'Tucker',\n",
      "       'Lola', 'Winston', 'Bo', 'the', 'Sadie', 'Buddy', 'Daisy', 'Toby',\n",
      "       'Bailey', 'an', 'Koda', 'Bella', 'Jax', 'Scout', 'Stanley', 'Jack',\n",
      "       'Leo', 'Dave', 'Rusty', 'Milo', 'Oscar', 'Alfie', 'Larry', 'Sammy',\n",
      "       'Phil', 'Chester', 'Sunny', 'George', 'Finn', 'Louis', 'Oakley',\n",
      "       'Bentley'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(archive_df.name.value_counts().index[0:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed above that **a,** **an**, **very**, **the** ... are not real dog names. Also there is a huge number of missing names. From what I can see above, it seems that most errors share a similar pattern, that is the first letter is in lowercase, as opposed to proper names.\n",
    "Let's develop this idea a bit further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quite</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getting</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mad</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unacceptable</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incredibly</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>officially</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>space</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infuriating</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>light</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>such</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name\n",
       "a               55\n",
       "the              8\n",
       "an               7\n",
       "very             5\n",
       "just             4\n",
       "quite            4\n",
       "one              4\n",
       "actually         2\n",
       "not              2\n",
       "getting          2\n",
       "mad              2\n",
       "unacceptable     1\n",
       "this             1\n",
       "incredibly       1\n",
       "by               1\n",
       "officially       1\n",
       "space            1\n",
       "my               1\n",
       "infuriating      1\n",
       "light            1\n",
       "such             1\n",
       "all              1\n",
       "old              1\n",
       "life             1\n",
       "his              1"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = pd.DataFrame(archive_df.name.value_counts())\n",
    "names.loc[[x.islower() for x in archive_df.name.value_counts().index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems clear that _name_ column requires some cleaning\n",
    "> **Some names are not real pet names**\n",
    "\n",
    "Also, I'm quite intrigued to know what does the source column represent. Loooking at tweeter's [api documentation](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html). Source gives information as to where data has been originated. Let's have a look at the values that this column takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>     2221\n",
       "<a href=\"http://vine.co\" rel=\"nofollow\">Vine - Make a Scene</a>                          91\n",
       "<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>                       33\n",
       "<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>      11\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_df.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that for this dataframe sample, only four values are possible. It can be therefore regarded as categorical data instead of the awful way with which this information is represented. Let's clean this column so categories are clearer when looking at this dataframe\n",
    "> **Source column contains categories in XML format**\n",
    "\n",
    "What about the ratings, we know the denominator needs to be always 10, but is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>retweeted_status_user_id</th>\n",
       "      <th>rating_numerator</th>\n",
       "      <th>rating_denominator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.356000e+03</td>\n",
       "      <td>7.700000e+01</td>\n",
       "      <td>7.700000e+01</td>\n",
       "      <td>1.810000e+02</td>\n",
       "      <td>1.810000e+02</td>\n",
       "      <td>2356.000000</td>\n",
       "      <td>2356.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.427716e+17</td>\n",
       "      <td>7.440692e+17</td>\n",
       "      <td>2.040329e+16</td>\n",
       "      <td>7.720400e+17</td>\n",
       "      <td>1.241698e+16</td>\n",
       "      <td>13.126486</td>\n",
       "      <td>10.455433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.856705e+16</td>\n",
       "      <td>7.524295e+16</td>\n",
       "      <td>1.260797e+17</td>\n",
       "      <td>6.236928e+16</td>\n",
       "      <td>9.599254e+16</td>\n",
       "      <td>45.876648</td>\n",
       "      <td>6.745237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.660209e+17</td>\n",
       "      <td>6.658147e+17</td>\n",
       "      <td>1.185634e+07</td>\n",
       "      <td>6.661041e+17</td>\n",
       "      <td>7.832140e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.783989e+17</td>\n",
       "      <td>6.757073e+17</td>\n",
       "      <td>3.589728e+08</td>\n",
       "      <td>7.186315e+17</td>\n",
       "      <td>4.196984e+09</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.196279e+17</td>\n",
       "      <td>7.032559e+17</td>\n",
       "      <td>4.196984e+09</td>\n",
       "      <td>7.804657e+17</td>\n",
       "      <td>4.196984e+09</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.993373e+17</td>\n",
       "      <td>8.233264e+17</td>\n",
       "      <td>4.196984e+09</td>\n",
       "      <td>8.203146e+17</td>\n",
       "      <td>4.196984e+09</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.924206e+17</td>\n",
       "      <td>8.862664e+17</td>\n",
       "      <td>8.405479e+17</td>\n",
       "      <td>8.874740e+17</td>\n",
       "      <td>7.874618e+17</td>\n",
       "      <td>1776.000000</td>\n",
       "      <td>170.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id  in_reply_to_status_id  in_reply_to_user_id  \\\n",
       "count  2.356000e+03           7.700000e+01         7.700000e+01   \n",
       "mean   7.427716e+17           7.440692e+17         2.040329e+16   \n",
       "std    6.856705e+16           7.524295e+16         1.260797e+17   \n",
       "min    6.660209e+17           6.658147e+17         1.185634e+07   \n",
       "25%    6.783989e+17           6.757073e+17         3.589728e+08   \n",
       "50%    7.196279e+17           7.032559e+17         4.196984e+09   \n",
       "75%    7.993373e+17           8.233264e+17         4.196984e+09   \n",
       "max    8.924206e+17           8.862664e+17         8.405479e+17   \n",
       "\n",
       "       retweeted_status_id  retweeted_status_user_id  rating_numerator  \\\n",
       "count         1.810000e+02              1.810000e+02       2356.000000   \n",
       "mean          7.720400e+17              1.241698e+16         13.126486   \n",
       "std           6.236928e+16              9.599254e+16         45.876648   \n",
       "min           6.661041e+17              7.832140e+05          0.000000   \n",
       "25%           7.186315e+17              4.196984e+09         10.000000   \n",
       "50%           7.804657e+17              4.196984e+09         11.000000   \n",
       "75%           8.203146e+17              4.196984e+09         12.000000   \n",
       "max           8.874740e+17              7.874618e+17       1776.000000   \n",
       "\n",
       "       rating_denominator  \n",
       "count         2356.000000  \n",
       "mean            10.455433  \n",
       "std              6.745237  \n",
       "min              0.000000  \n",
       "25%             10.000000  \n",
       "50%             10.000000  \n",
       "75%             10.000000  \n",
       "max            170.000000  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that denominator includes at least one vlaue of 170. Also a value of 0. This is wrong as we expect all denominators to be 10. Let's see what is the text for those denominators != 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"@jonnysun @Lin_Manuel ok jomny I know you're excited but 960/00 isn't a valid rating, 13/10 is tho\",\n",
       "       '@docmisterio account started on 11/15/15',\n",
       "       'The floofs have been released I repeat the floofs have been released. 84/70 https://t.co/NIYC820tmd',\n",
       "       'Meet Sam. She smiles 24/7 &amp; secretly aspires to be a reindeer. \\nKeep Sam smiling by clicking and sharing this link:\\nhttps://t.co/98tB8y7y7t https://t.co/LouL5vdvxx',\n",
       "       'RT @dog_rates: After so many requests, this is Bretagne. She was the last surviving 9/11 search dog, and our second ever 14/10. RIP https:/…'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_df[ archive_df['rating_denominator'] != 10 ].text.values[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a few issues with the denominator that we will need to address in the cleaning stage. Also the numerator seems to showcast similar stats, let's see a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['This is Bella. She hopes her smile made you smile. If not, she is also offering you her favorite monkey. 13.5/10 https://t.co/qjrljjt948',\n",
       "       '@dhmontgomery We also gave snoop dogg a 420/10 but I think that predated your research',\n",
       "       '@s8n You tried very hard to portray this good boy as not so good, but you have ultimately failed. His goodness shines through. 666/10',\n",
       "       \"This is Jerry. He's doing a distinguished tongue slip. Slightly patronizing tbh. You think you're better than us, Jerry? 6/10 hold me back https://t.co/DkOBbwulw1\",\n",
       "       '@markhoppus 182/10'], dtype=object)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_df[ (archive_df['rating_numerator'] > 20) | (archive_df['rating_numerator'] < 10)].text.values[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerator is harder to clean... sometimes it's not clear whether it was a typing error or an intentional high or low score... However instances like the first one that contains decimal numbers are not properly parsed and needs fixing\n",
    "> **Denominator column contains non 10 values**\n",
    "\n",
    "> **Numerator has not been properly parsed in some cases**\n",
    "\n",
    "Also when looking at dog categories, it can be seen that data is **messy**. We find that there are four columns that can be grouped into one as they are referring to the same variable.\n",
    "> **Archive_df contains messy dog category data**\n",
    "\n",
    "Let's now have a look at the second dataframe, that is image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1_conf</th>\n",
       "      <th>p1_dog</th>\n",
       "      <th>p2</th>\n",
       "      <th>p2_conf</th>\n",
       "      <th>p2_dog</th>\n",
       "      <th>p3</th>\n",
       "      <th>p3_conf</th>\n",
       "      <th>p3_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>666020888022790149</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Welsh_springer_spaniel</td>\n",
       "      <td>0.465074</td>\n",
       "      <td>True</td>\n",
       "      <td>collie</td>\n",
       "      <td>0.156665</td>\n",
       "      <td>True</td>\n",
       "      <td>Shetland_sheepdog</td>\n",
       "      <td>0.061428</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>666029285002620928</td>\n",
       "      <td>https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>redbone</td>\n",
       "      <td>0.506826</td>\n",
       "      <td>True</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.074192</td>\n",
       "      <td>True</td>\n",
       "      <td>Rhodesian_ridgeback</td>\n",
       "      <td>0.072010</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>666033412701032449</td>\n",
       "      <td>https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>German_shepherd</td>\n",
       "      <td>0.596461</td>\n",
       "      <td>True</td>\n",
       "      <td>malinois</td>\n",
       "      <td>0.138584</td>\n",
       "      <td>True</td>\n",
       "      <td>bloodhound</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>666044226329800704</td>\n",
       "      <td>https://pbs.twimg.com/media/CT5Dr8HUEAA-lEu.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Rhodesian_ridgeback</td>\n",
       "      <td>0.408143</td>\n",
       "      <td>True</td>\n",
       "      <td>redbone</td>\n",
       "      <td>0.360687</td>\n",
       "      <td>True</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>666049248165822465</td>\n",
       "      <td>https://pbs.twimg.com/media/CT5IQmsXIAAKY4A.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "      <td>0.560311</td>\n",
       "      <td>True</td>\n",
       "      <td>Rottweiler</td>\n",
       "      <td>0.243682</td>\n",
       "      <td>True</td>\n",
       "      <td>Doberman</td>\n",
       "      <td>0.154629</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                          jpg_url  \\\n",
       "0  666020888022790149  https://pbs.twimg.com/media/CT4udn0WwAA0aMy.jpg   \n",
       "1  666029285002620928  https://pbs.twimg.com/media/CT42GRgUYAA5iDo.jpg   \n",
       "2  666033412701032449  https://pbs.twimg.com/media/CT4521TWwAEvMyu.jpg   \n",
       "3  666044226329800704  https://pbs.twimg.com/media/CT5Dr8HUEAA-lEu.jpg   \n",
       "4  666049248165822465  https://pbs.twimg.com/media/CT5IQmsXIAAKY4A.jpg   \n",
       "\n",
       "   img_num                      p1   p1_conf  p1_dog                  p2  \\\n",
       "0        1  Welsh_springer_spaniel  0.465074    True              collie   \n",
       "1        1                 redbone  0.506826    True  miniature_pinscher   \n",
       "2        1         German_shepherd  0.596461    True            malinois   \n",
       "3        1     Rhodesian_ridgeback  0.408143    True             redbone   \n",
       "4        1      miniature_pinscher  0.560311    True          Rottweiler   \n",
       "\n",
       "    p2_conf  p2_dog                   p3   p3_conf  p3_dog  \n",
       "0  0.156665    True    Shetland_sheepdog  0.061428    True  \n",
       "1  0.074192    True  Rhodesian_ridgeback  0.072010    True  \n",
       "2  0.138584    True           bloodhound  0.116197    True  \n",
       "3  0.360687    True   miniature_pinscher  0.222752    True  \n",
       "4  0.243682    True             Doberman  0.154629    True  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's fair to say that this dataframe shows issues with structure. p1, p2 and p3 refer to the same variable. The same applies to pX_conf and pX_dog where X in [1,2,3]. Therefore this dataframe needs cleaning\n",
    "> **Same variable spread across a few columns**\n",
    "\n",
    "Also, why are there columns that indicate whether the prediction is a dog or not. Let's have a look at what happens when some of these are wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>jpg_url</th>\n",
       "      <th>img_num</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1_conf</th>\n",
       "      <th>p1_dog</th>\n",
       "      <th>p2</th>\n",
       "      <th>p2_conf</th>\n",
       "      <th>p2_dog</th>\n",
       "      <th>p3</th>\n",
       "      <th>p3_conf</th>\n",
       "      <th>p3_dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>666051853826850816</td>\n",
       "      <td>https://pbs.twimg.com/media/CT5KoJ1WoAAJash.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>box_turtle</td>\n",
       "      <td>0.933012</td>\n",
       "      <td>False</td>\n",
       "      <td>mud_turtle</td>\n",
       "      <td>0.045885</td>\n",
       "      <td>False</td>\n",
       "      <td>terrapin</td>\n",
       "      <td>0.017885</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>666055525042405380</td>\n",
       "      <td>https://pbs.twimg.com/media/CT5N9tpXIAAifs1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>chow</td>\n",
       "      <td>0.692517</td>\n",
       "      <td>True</td>\n",
       "      <td>Tibetan_mastiff</td>\n",
       "      <td>0.058279</td>\n",
       "      <td>True</td>\n",
       "      <td>fur_coat</td>\n",
       "      <td>0.054449</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>666057090499244032</td>\n",
       "      <td>https://pbs.twimg.com/media/CT5PY90WoAAQGLo.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>shopping_cart</td>\n",
       "      <td>0.962465</td>\n",
       "      <td>False</td>\n",
       "      <td>shopping_basket</td>\n",
       "      <td>0.014594</td>\n",
       "      <td>False</td>\n",
       "      <td>golden_retriever</td>\n",
       "      <td>0.007959</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>666104133288665088</td>\n",
       "      <td>https://pbs.twimg.com/media/CT56LSZWoAAlJj2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>hen</td>\n",
       "      <td>0.965932</td>\n",
       "      <td>False</td>\n",
       "      <td>cock</td>\n",
       "      <td>0.033919</td>\n",
       "      <td>False</td>\n",
       "      <td>partridge</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>666268910803644416</td>\n",
       "      <td>https://pbs.twimg.com/media/CT8QCd1WEAADXws.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>desktop_computer</td>\n",
       "      <td>0.086502</td>\n",
       "      <td>False</td>\n",
       "      <td>desk</td>\n",
       "      <td>0.085547</td>\n",
       "      <td>False</td>\n",
       "      <td>bookcase</td>\n",
       "      <td>0.079480</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                          jpg_url  \\\n",
       "6   666051853826850816  https://pbs.twimg.com/media/CT5KoJ1WoAAJash.jpg   \n",
       "7   666055525042405380  https://pbs.twimg.com/media/CT5N9tpXIAAifs1.jpg   \n",
       "8   666057090499244032  https://pbs.twimg.com/media/CT5PY90WoAAQGLo.jpg   \n",
       "17  666104133288665088  https://pbs.twimg.com/media/CT56LSZWoAAlJj2.jpg   \n",
       "18  666268910803644416  https://pbs.twimg.com/media/CT8QCd1WEAADXws.jpg   \n",
       "\n",
       "    img_num                p1   p1_conf  p1_dog               p2   p2_conf  \\\n",
       "6         1        box_turtle  0.933012   False       mud_turtle  0.045885   \n",
       "7         1              chow  0.692517    True  Tibetan_mastiff  0.058279   \n",
       "8         1     shopping_cart  0.962465   False  shopping_basket  0.014594   \n",
       "17        1               hen  0.965932   False             cock  0.033919   \n",
       "18        1  desktop_computer  0.086502   False             desk  0.085547   \n",
       "\n",
       "    p2_dog                p3   p3_conf  p3_dog  \n",
       "6    False          terrapin  0.017885   False  \n",
       "7     True          fur_coat  0.054449   False  \n",
       "8    False  golden_retriever  0.007959    True  \n",
       "17   False         partridge  0.000052   False  \n",
       "18   False          bookcase  0.079480   False  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df[ (image_df['p1_dog'] == False) | (image_df['p2_dog'] == False) | (image_df['p3_dog'] == False) ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that some of the images are either not a dog, or contain an object which is not a dog, or the algorithm prediction is wrong. In either case, and since we are dealing with data that relates to dogs, then I think it's cleaner to have data with dogs-only predictions\n",
    "> **Some predictions are not dogs**\n",
    "\n",
    "As for the last dataframe tweeter_df it belongs to the same observational unit as archive_df so we should _join_ both of them\n",
    "> **Twitter_df same observational unit as archive_df**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality\n",
    "- Some retweets contain duplicate information\n",
    "- Timestamp columns are strings\n",
    "- Some replies contain duplicate information\n",
    "- Expanded_urls contain missing information\n",
    "- Some names are not pet names\n",
    "- Denominator contains non 10 values\n",
    "- Numerator parsing is not entirely correct\n",
    "- Some predictions are not dogs\n",
    "- Source column contains categories in XML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness\n",
    "- Dog category is messy\n",
    "- Image_df same variable uses a few columns\n",
    "- Twitter_df belongs to the same observational unit as archive_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id='clean'></a>\n",
    "## Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Some retweets contain duplicate information \n",
    " \n",
    "So we had concluded that some of the retweets present in archive_df were retweets for tweets that were already present in archive_df. Therefore, this in turn leads to the same dog being rated twice, let's remove the retweets and keep the original. Also, remember that the problem with retweets is that the column's type was float and so some information was lost because of that cast. Full data was fetched from tweetjson.txt_ and kept in a new column retweeted_status_id_2. Lets drop the column with the original results in float and rename the one that contains accurate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweet id for which the tweet was a retweet\n",
    "archive_df_clean = archive_df.copy()\n",
    "archive_df_clean.drop(['retweeted_status_id'],axis=1,inplace=True)\n",
    "archive_df_clean.rename(columns={'retweeted_status_id_2':'retweeted_status_id'},inplace=True)\n",
    "retweets = archive_df_clean[archive_df_clean.retweeted_status_id.notnull()]\n",
    "duplicated = np.isin(retweets['retweeted_status_id'].astype('int64').values,archive_df_clean.tweet_id.values)\n",
    "retweets = retweets[duplicated]\n",
    "archive_df_clean.drop(retweets.index,inplace=True)##### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that duplicated rows have been removed. Remember that we found out before there were 145 tweets that were retweets for which the original tweet was present in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in columns between cleaned df and original df: 145\n"
     ]
    }
   ],
   "source": [
    "print('Difference in columns between cleaned df and original df: {}'.format(archive_df.shape[0]-archive_df_clean.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Timestamp columns are strings, user ids are float\n",
    " \n",
    "Similar to what we disovered ealier, some column types are plainly wrong. Some are useless (as it was the case before) and require data gathering again. First, let's change _timestamp_ and *retweeted_status_timestamp* to **datetime**. Second let's change *in_reply_to_user_id* and *retweeted_status_user_id* to strings. Unlike tweet_ids, user IDs are much smaller numbers and so no information has been lost. It should be sufficient to cast the columns to the appropriate type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df_clean['timestamp'] = pd.to_datetime(archive_df_clean.timestamp)\n",
    "archive_df_clean['retweetebd_status_timestamp'] = pd.to_datetime(archive_df_clean.retweeted_status_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df_clean.in_reply_to_user_id = archive_df_clean['in_reply_to_user_id'].astype(str).str.extract('(\\d+)')\n",
    "archive_df_clean.retweeted_status_user_id = archive_df_clean['retweeted_status_user_id'].astype(str).str.extract('(\\d+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that types for the columns that were casted have properly adopted the new types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2211 entries, 0 to 2355\n",
      "Data columns (total 19 columns):\n",
      "tweet_id                       2211 non-null int64\n",
      "in_reply_to_status_id          77 non-null float64\n",
      "in_reply_to_user_id            77 non-null object\n",
      "timestamp                      2211 non-null datetime64[ns]\n",
      "source                         2211 non-null object\n",
      "text                           2211 non-null object\n",
      "retweeted_status_user_id       36 non-null object\n",
      "retweeted_status_timestamp     36 non-null object\n",
      "expanded_urls                  2152 non-null object\n",
      "rating_numerator               2211 non-null int64\n",
      "rating_denominator             2211 non-null int64\n",
      "name                           2211 non-null object\n",
      "doggo                          2211 non-null object\n",
      "floofer                        2211 non-null object\n",
      "pupper                         2211 non-null object\n",
      "puppo                          2211 non-null object\n",
      "retweeted_status_id            23 non-null object\n",
      "in_reply_to_status_id_2        77 non-null object\n",
      "retweetebd_status_timestamp    36 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(1), int64(3), object(13)\n",
      "memory usage: 345.5+ KB\n"
     ]
    }
   ],
   "source": [
    "archive_df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Some replies contain duplicate information \n",
    "\n",
    "Similarly to retweets that contain information somehow duplicated from an original tweet, replies may also show this problem.\n",
    "So it may well be that there are replies that address tweets that are already part of the archive_df. During assessment it was observed that there are 44 of these replies that need be removed. All data was already gathered and stored into *in_reply_to_status_id_2* and so we need to rename this column and drop the original. Then, proceed with removing redundant information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweet id for which the tweet was a reply\n",
    "num_rows_before_cleaning_replies = archive_df_clean.shape[0]\n",
    "archive_df_clean.drop(['in_reply_to_status_id'],axis=1,inplace=True)\n",
    "archive_df_clean.rename(columns={'in_reply_to_status_id_2':'in_reply_to_status_id'},inplace=True)\n",
    "replies = archive_df_clean[archive_df_clean.in_reply_to_status_id.notnull()]\n",
    "duplicated = np.isin(replies['in_reply_to_status_id'].astype('int64').values,archive_df_clean.tweet_id.values)\n",
    "replies = replies[duplicated]\n",
    "archive_df_clean.drop(replies.index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that duplicated rows have been removed. Remember that we found out before there were 44 tweets that were replies to tweets that were already present in the dataframe. Therefore we expect the number of rows to have decreased by 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in columns between cleaned df and original df: 44\n"
     ]
    }
   ],
   "source": [
    "print('Difference in columns between cleaned df and original df: {}'.format(num_rows_before_cleaning_replies-archive_df_clean.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Expanded_urls contain missing information \n",
    "\n",
    "It was concluded earlier that some information in column *expanded_urls* was missing. Let's collect this information from **tweet_json.txt** and fill in the missing gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweet id for which the tweet was a reply\n",
    "listTweetsNoUrl = archive_df_clean[archive_df_clean['expanded_urls'].isnull()].tweet_id.values\n",
    "\n",
    "with open('tweet_json.txt','r',encoding='utf-8') as fjsonRead:\n",
    "    for line in fjsonRead:\n",
    "        json_dict = json.loads(line)\n",
    "        if json_dict['id'] in listTweetsNoUrl:\n",
    "            archive_df_clean.loc[archive_df_clean[archive_df_clean['tweet_id']==json_dict['id']].index,'expanded_urls'] = json_dict['user']['entities']['url']['urls'][0]['expanded_url']            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can observe that there are missing values in column expanded_urls **no more**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2167 entries, 0 to 2355\n",
      "Data columns (total 18 columns):\n",
      "tweet_id                       2167 non-null int64\n",
      "in_reply_to_user_id            33 non-null object\n",
      "timestamp                      2167 non-null datetime64[ns]\n",
      "source                         2167 non-null object\n",
      "text                           2167 non-null object\n",
      "retweeted_status_user_id       36 non-null object\n",
      "retweeted_status_timestamp     36 non-null object\n",
      "expanded_urls                  2167 non-null object\n",
      "rating_numerator               2167 non-null int64\n",
      "rating_denominator             2167 non-null int64\n",
      "name                           2167 non-null object\n",
      "doggo                          2167 non-null object\n",
      "floofer                        2167 non-null object\n",
      "pupper                         2167 non-null object\n",
      "puppo                          2167 non-null object\n",
      "retweeted_status_id            23 non-null object\n",
      "in_reply_to_status_id          33 non-null object\n",
      "retweetebd_status_timestamp    36 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(3), object(13)\n",
      "memory usage: 401.7+ KB\n"
     ]
    }
   ],
   "source": [
    "archive_df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Some names are not pet names\n",
    "As it was observed earlier in the document, some of the parsed names do not correspond to pet names or names at all. The pattern that most of them share is that they are lower case. In order to clean this, let's show a couple of examples to determine what was the regex used to extract the name, and whether it's possible to improve it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names that start with lowercase 105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I've yet to rate a Venezuelan Hover Wiener. This is such an honor. 14/10 paw-inspiring af (IG: roxy.thedoxy) https://t.co/20VrLAA8ba</td>\n",
       "      <td>such</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Here is a pupper approaching maximum borkdrive. Zooming at never before seen speeds. 14/10 paw-inspiring af \\n(IG: puffie_the_chow) https://t.co/ghXBIIeQZF</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>RT @dog_rates: We only rate dogs. This is quite clearly a smol broken polar bear. We'd appreciate if you only send dogs. Thank you... 12/10…</td>\n",
       "      <td>quite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>We only rate dogs. This is quite clearly a smol broken polar bear. We'd appreciate if you only send dogs. Thank you... 12/10 https://t.co/g2nSyGenG9</td>\n",
       "      <td>quite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Guys, we only rate dogs. This is quite clearly a bulbasaur. Please only send dogs. Thank you... 12/10 human used pet, it's super effective https://t.co/Xc7uj1C64x</td>\n",
       "      <td>quite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>There's going to be a dog terminal at JFK Airport. This is not a drill. 10/10  \\nhttps://t.co/dp5h9bCwU7</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>Occasionally, we're sent fantastic stories. This is one of them. 14/10 for Grace https://t.co/bZ4axuH6OK</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>We only rate dogs. Please stop sending in non-canines like this Freudian Poof Lion. This is incredibly frustrating... 11/10 https://t.co/IZidSrBvhi</td>\n",
       "      <td>incredibly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>Here is a perfect example of someone who has their priorities in order. 13/10 for both owner and Forrest https://t.co/LRyMrU7Wfq</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>Guys this is getting so out of hand. We only rate dogs. This is a Galapagos Speed Panda. Pls only send dogs... 10/10 https://t.co/8lpAGaZRFn</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>We only rate dogs. Pls stop sending in non-canines like this Arctic Floof Kangaroo. This is very frustrating. 11/10 https://t.co/qlUDuPoE3d</td>\n",
       "      <td>very</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>This is my dog. Her name is Zoey. She knows I've been rating other dogs. She's not happy. 13/10 no bias at all https://t.co/ep1NkYoiwB</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>This is one of the most inspirational stories I've ever come across. I have no words. 14/10 for both doggo and owner https://t.co/I5ld3eKD5k</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>What jokester sent in a pic without a dog in it? This is not @rock_rates. This is @dog_rates. Thank you ...10/10 https://t.co/nDPaYHrtNX</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>That is Quizno. This is his beach. He does not tolerate human shenanigans on his beach. 10/10 reclaim ur land doggo https://t.co/vdr7DaRSa7</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>This is one of the most reckless puppers I've ever seen. How she got a license in the first place is beyond me. 6/10 https://t.co/z5bAdtn9kd</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>This is a mighty rare blue-tailed hammer sherk. Human almost lost a limb trying to take these. Be careful guys. 8/10 https://t.co/TGenMeXreW</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>Viewer discretion is advised. This is a terrible attack in progress. Not even in water (tragic af). 4/10 bad sherk https://t.co/L3U0j14N5R</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>This is a carrot. We only rate dogs. Please only send in dogs. You all really should know this by now ...11/10 https://t.co/9e48aPrBm2</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>This is an Iraqi Speed Kangaroo. It is not a dog. Please only send in dogs. I'm very angry with all of you ...9/10 https://t.co/5qpBTTpgUt</td>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                    text  \\\n",
       "22                                  I've yet to rate a Venezuelan Hover Wiener. This is such an honor. 14/10 paw-inspiring af (IG: roxy.thedoxy) https://t.co/20VrLAA8ba   \n",
       "56           Here is a pupper approaching maximum borkdrive. Zooming at never before seen speeds. 14/10 paw-inspiring af \\n(IG: puffie_the_chow) https://t.co/ghXBIIeQZF   \n",
       "118                         RT @dog_rates: We only rate dogs. This is quite clearly a smol broken polar bear. We'd appreciate if you only send dogs. Thank you... 12/10…   \n",
       "169                 We only rate dogs. This is quite clearly a smol broken polar bear. We'd appreciate if you only send dogs. Thank you... 12/10 https://t.co/g2nSyGenG9   \n",
       "193   Guys, we only rate dogs. This is quite clearly a bulbasaur. Please only send dogs. Thank you... 12/10 human used pet, it's super effective https://t.co/Xc7uj1C64x   \n",
       "335                                                             There's going to be a dog terminal at JFK Airport. This is not a drill. 10/10  \\nhttps://t.co/dp5h9bCwU7   \n",
       "369                                                             Occasionally, we're sent fantastic stories. This is one of them. 14/10 for Grace https://t.co/bZ4axuH6OK   \n",
       "542                  We only rate dogs. Please stop sending in non-canines like this Freudian Poof Lion. This is incredibly frustrating... 11/10 https://t.co/IZidSrBvhi   \n",
       "649                                     Here is a perfect example of someone who has their priorities in order. 13/10 for both owner and Forrest https://t.co/LRyMrU7Wfq   \n",
       "801                         Guys this is getting so out of hand. We only rate dogs. This is a Galapagos Speed Panda. Pls only send dogs... 10/10 https://t.co/8lpAGaZRFn   \n",
       "819                          We only rate dogs. Pls stop sending in non-canines like this Arctic Floof Kangaroo. This is very frustrating. 11/10 https://t.co/qlUDuPoE3d   \n",
       "852                               This is my dog. Her name is Zoey. She knows I've been rating other dogs. She's not happy. 13/10 no bias at all https://t.co/ep1NkYoiwB   \n",
       "924                         This is one of the most inspirational stories I've ever come across. I have no words. 14/10 for both doggo and owner https://t.co/I5ld3eKD5k   \n",
       "988                             What jokester sent in a pic without a dog in it? This is not @rock_rates. This is @dog_rates. Thank you ...10/10 https://t.co/nDPaYHrtNX   \n",
       "992                          That is Quizno. This is his beach. He does not tolerate human shenanigans on his beach. 10/10 reclaim ur land doggo https://t.co/vdr7DaRSa7   \n",
       "993                         This is one of the most reckless puppers I've ever seen. How she got a license in the first place is beyond me. 6/10 https://t.co/z5bAdtn9kd   \n",
       "1002                        This is a mighty rare blue-tailed hammer sherk. Human almost lost a limb trying to take these. Be careful guys. 8/10 https://t.co/TGenMeXreW   \n",
       "1004                          Viewer discretion is advised. This is a terrible attack in progress. Not even in water (tragic af). 4/10 bad sherk https://t.co/L3U0j14N5R   \n",
       "1017                              This is a carrot. We only rate dogs. Please only send in dogs. You all really should know this by now ...11/10 https://t.co/9e48aPrBm2   \n",
       "1025                          This is an Iraqi Speed Kangaroo. It is not a dog. Please only send in dogs. I'm very angry with all of you ...9/10 https://t.co/5qpBTTpgUt   \n",
       "\n",
       "            name  \n",
       "22          such  \n",
       "56             a  \n",
       "118        quite  \n",
       "169        quite  \n",
       "193        quite  \n",
       "335          not  \n",
       "369          one  \n",
       "542   incredibly  \n",
       "649            a  \n",
       "801            a  \n",
       "819         very  \n",
       "852           my  \n",
       "924          one  \n",
       "988          not  \n",
       "992          his  \n",
       "993          one  \n",
       "1002           a  \n",
       "1004           a  \n",
       "1017           a  \n",
       "1025          an  "
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopetname_df = archive_df_clean[archive_df_clean.name.apply(lambda x: x.islower())]\n",
    "pd.set_option('max_colwidth',200)\n",
    "print('Number of names that start with lowercase {}'.format(nopetname_df.shape[0]))\n",
    "nopetname_df[['text','name']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that some of the names are after the accronym IG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alejanma\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "reg = '([I,i][G,g]: (\\w+))'\n",
    "newnames = nopetname_df.text.str.extract(reg)[1]\n",
    "newnames = newnames[newnames.notnull()]\n",
    "newnames = newnames.apply(lambda x: x[0].upper() + x[1:])\n",
    "archive_df_clean.loc[newnames.index,'name'] = newnames.values\n",
    "nopetname_df.drop(newnames.index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at those rows that contain name on them to see if we can find a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>This is my dog. Her name is Zoey. She knows I've been rating other dogs. She's not happy. 13/10 no bias at all https://t.co/ep1NkYoiwB</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>This is a Sizzlin Menorah spaniel from Brooklyn named Wylie. Lovable eyes. Chiller as hell. 10/10 and I'm out.. poof https://t.co/7E0AiJXPmI</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>This is a Lofted Aphrodisiac Terrier named Kip. Big fan of bed n breakfasts. Fits perfectly. 10/10 would pet firmly https://t.co/gKlLpNzIl3</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>This is a Tuscaloosa Alcatraz named Jacob (Yacōb). Loves to sit in swing. Stellar tongue. 11/10 look at his feet https://t.co/2IslQ8ZSc7</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>This is a Helvetica Listerine named Rufus. This time Rufus will be ready for the UPS guy. He'll never expect it 9/10 https://t.co/34OhVhMkVr</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>This is a Deciduous Trimester mix named Spork. Only 1 ear works. No seat belt. Incredibly reckless. 9/10 still cute https://t.co/CtuJoLHiDo</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>This is a Rich Mahogany Seltzer named Cherokee. Just got destroyed by a snowball. Isn't very happy about it. 9/10 https://t.co/98ZBi6o4dj</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>This is a Speckled Cauliflower Yosemite named Hemry. He's terrified of intruder dog. Not one bit comfortable. 9/10 https://t.co/yV3Qgjh8iN</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>This is a spotted Lipitor Rumpelstiltskin named Alphred. He can't wait for the Turkey. 10/10 would pet really well https://t.co/6GUGO7azNX</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>This is a Coriander Baton Rouge named Alfredo. Loves to cuddle with smaller well-dressed dog. 10/10 would hug lots https://t.co/eCRdwouKCl</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              text  \\\n",
       "852         This is my dog. Her name is Zoey. She knows I've been rating other dogs. She's not happy. 13/10 no bias at all https://t.co/ep1NkYoiwB   \n",
       "1853  This is a Sizzlin Menorah spaniel from Brooklyn named Wylie. Lovable eyes. Chiller as hell. 10/10 and I'm out.. poof https://t.co/7E0AiJXPmI   \n",
       "1955   This is a Lofted Aphrodisiac Terrier named Kip. Big fan of bed n breakfasts. Fits perfectly. 10/10 would pet firmly https://t.co/gKlLpNzIl3   \n",
       "2034      This is a Tuscaloosa Alcatraz named Jacob (Yacōb). Loves to sit in swing. Stellar tongue. 11/10 look at his feet https://t.co/2IslQ8ZSc7   \n",
       "2066  This is a Helvetica Listerine named Rufus. This time Rufus will be ready for the UPS guy. He'll never expect it 9/10 https://t.co/34OhVhMkVr   \n",
       "2116   This is a Deciduous Trimester mix named Spork. Only 1 ear works. No seat belt. Incredibly reckless. 9/10 still cute https://t.co/CtuJoLHiDo   \n",
       "2125     This is a Rich Mahogany Seltzer named Cherokee. Just got destroyed by a snowball. Isn't very happy about it. 9/10 https://t.co/98ZBi6o4dj   \n",
       "2128    This is a Speckled Cauliflower Yosemite named Hemry. He's terrified of intruder dog. Not one bit comfortable. 9/10 https://t.co/yV3Qgjh8iN   \n",
       "2146    This is a spotted Lipitor Rumpelstiltskin named Alphred. He can't wait for the Turkey. 10/10 would pet really well https://t.co/6GUGO7azNX   \n",
       "2161    This is a Coriander Baton Rouge named Alfredo. Loves to cuddle with smaller well-dressed dog. 10/10 would hug lots https://t.co/eCRdwouKCl   \n",
       "\n",
       "     name  \n",
       "852    my  \n",
       "1853    a  \n",
       "1955    a  \n",
       "2034    a  \n",
       "2066    a  \n",
       "2116    a  \n",
       "2125    a  \n",
       "2128    a  \n",
       "2146    a  \n",
       "2161    a  "
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noname = nopetname_df.loc [ nopetname_df.text.str.extract('([n,N]ame)').notnull().iloc[:,0].values]\n",
    "pd.set_option('max_colwidth',200)\n",
    "noname.loc[:,['text','name']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a pattern above, that is \"named ...\". Let's change those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "newnames = nopetname_df.text.str.extract('([n,N]amed\\s)(\\w+)')[1]\n",
    "newnames = newnames[ newnames.notnull()]\n",
    "archive_df_clean.loc[newnames.index,'name'] = newnames.values\n",
    "nopetname_df.drop(newnames.index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another pattern that can be observed is \"name is ...\" Let's fix that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 18)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newnames = nopetname_df.text.str.extract('([n,N]ame\\sis\\s)(\\w+)')[1]\n",
    "newnames = newnames[ newnames.notnull()]\n",
    "archive_df_clean.loc[newnames.index,'name'] = newnames.values\n",
    "nopetname_df.drop(newnames.index,inplace=True)\n",
    "nopetname_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest are quite hard to find a patter for... so let's change the wrong name to NaN. The same applies for all those that have been assigned a name == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "indextoChange = archive_df_clean.loc[archive_df_clean['name'] == 'None'].index\n",
    "archive_df_clean.loc[indextoChange,'name'] = np.NaN\n",
    "archive_df_clean.loc[nopetname_df.index,'name'] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether there are names that start with lowercase. Also let's check that some rows are NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name]\n",
       "Index: []"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = pd.DataFrame(archive_df_clean.name.value_counts())\n",
    "names.loc[[x.islower() for x in archive_df_clean.name.value_counts().index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2211 entries, 0 to 2355\n",
      "Data columns (total 18 columns):\n",
      "tweet_id                      2211 non-null int64\n",
      "in_reply_to_status_id         77 non-null float64\n",
      "in_reply_to_user_id           77 non-null float64\n",
      "timestamp                     2211 non-null object\n",
      "source                        2211 non-null object\n",
      "text                          2211 non-null object\n",
      "retweeted_status_user_id      36 non-null float64\n",
      "retweeted_status_timestamp    36 non-null object\n",
      "expanded_urls                 2152 non-null object\n",
      "rating_numerator              2211 non-null int64\n",
      "rating_denominator            2211 non-null int64\n",
      "name                          1421 non-null object\n",
      "doggo                         2211 non-null object\n",
      "floofer                       2211 non-null object\n",
      "pupper                        2211 non-null object\n",
      "puppo                         2211 non-null object\n",
      "retweeted_status_id           23 non-null object\n",
      "in_reply_to_status_id_2       77 non-null object\n",
      "dtypes: float64(3), int64(3), object(12)\n",
      "memory usage: 408.2+ KB\n"
     ]
    }
   ],
   "source": [
    "archive_df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Denominator contains non 10 values\n",
    "Denominator should be 10 all the time. So cases where this is not 10 may indicate problems with parsing that may well affect the numerator as well. So first let's check how many non 10 denominators there are. Also, let's print \"text\" to see why this error may have happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with wrong denominator: 23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating_denominator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>@jonnysun @Lin_Manuel ok jomny I know you're excited but 960/00 isn't a valid rating, 13/10 is tho</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>@docmisterio account started on 11/15/15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>The floofs have been released I repeat the floofs have been released. 84/70 https://t.co/NIYC820tmd</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>Meet Sam. She smiles 24/7 &amp;amp; secretly aspires to be a reindeer. \\nKeep Sam smiling by clicking and sharing this link:\\nhttps://t.co/98tB8y7y7t https://t.co/LouL5vdvxx</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>RT @dog_rates: After so many requests, this is Bretagne. She was the last surviving 9/11 search dog, and our second ever 14/10. RIP https:/…</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>Why does this never happen at my front door... 165/150 https://t.co/HmwrdfEfUE</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>After so many requests, this is Bretagne. She was the last surviving 9/11 search dog, and our second ever 14/10. RIP https://t.co/XAVDNDaVgQ</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>Say hello to this unbelievably well behaved squad of doggos. 204/170 would try to pet all at once https://t.co/yGQI3He3xv</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>Happy 4/20 from the squad! 13/10 for all https://t.co/eV1diwds8a</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>This is Bluebert. He just saw that both #FinalFur match ups are split 50/50. Amazed af. 11/10 https://t.co/Kky1DPG4iq</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>Happy Saturday here's 9 puppers on a bench. 99/90 good work everybody https://t.co/mpvaVxKmc1</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>Here's a brigade of puppers. All look very prepared for whatever happens next. 80/80 https://t.co/0eb7R1Om12</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>From left to right:\\nCletus, Jerome, Alejandro, Burp, &amp;amp; Titson\\nNone know where camera is. 45/50 would hug all at once https://t.co/sedre1ivTK</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>Here is a whole flock of puppers.  60/50 I'll take the lot https://t.co/9dpcw6MdWa</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>Happy Wednesday here's a bucket of pups. 44/40 would pet all at once https://t.co/HppvrYuamZ</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>Yes I do realize a rating of 4/20 would've been fitting. However, it would be unjust to give these cooperative pups that low of a rating</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>Two sneaky puppers were not initially seen, moving the rating to 143/130. Please forgive us. Thank you https://t.co/kRK51Y5ac3</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>Someone help the girl is being mugged. Several are distracting her while two steal her shoes. Clever puppers 121/110 https://t.co/1zfnTJLt55</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>This is Darrel. He just robbed a 7/11 and is in a high speed police chase. Was just spotted by the helicopter 10/10 https://t.co/7EsP8LmSp5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>I'm aware that I could've said 20/16, but here at WeRateDogs we are very professional. An inconsistent rating scale is simply irresponsible</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>IT'S PUPPERGEDDON. Total of 144/120 ...I think https://t.co/ZanVtAtvIq</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>Here we have an entire platoon of puppers. Total score: 88/80 would pet all at once https://t.co/y93p6FLvVw</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>This is an Albanian 3 1/2 legged  Episcopalian. Loves well-polished hardwood flooring. Penis on the collar. 9/10 https://t.co/d9NcXFKwLv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                           text  \\\n",
       "313                                                                          @jonnysun @Lin_Manuel ok jomny I know you're excited but 960/00 isn't a valid rating, 13/10 is tho   \n",
       "342                                                                                                                                    @docmisterio account started on 11/15/15   \n",
       "433                                                                         The floofs have been released I repeat the floofs have been released. 84/70 https://t.co/NIYC820tmd   \n",
       "516   Meet Sam. She smiles 24/7 &amp; secretly aspires to be a reindeer. \\nKeep Sam smiling by clicking and sharing this link:\\nhttps://t.co/98tB8y7y7t https://t.co/LouL5vdvxx   \n",
       "784                                RT @dog_rates: After so many requests, this is Bretagne. She was the last surviving 9/11 search dog, and our second ever 14/10. RIP https:/…   \n",
       "902                                                                                              Why does this never happen at my front door... 165/150 https://t.co/HmwrdfEfUE   \n",
       "1068                               After so many requests, this is Bretagne. She was the last surviving 9/11 search dog, and our second ever 14/10. RIP https://t.co/XAVDNDaVgQ   \n",
       "1120                                                  Say hello to this unbelievably well behaved squad of doggos. 204/170 would try to pet all at once https://t.co/yGQI3He3xv   \n",
       "1165                                                                                                           Happy 4/20 from the squad! 13/10 for all https://t.co/eV1diwds8a   \n",
       "1202                                                      This is Bluebert. He just saw that both #FinalFur match ups are split 50/50. Amazed af. 11/10 https://t.co/Kky1DPG4iq   \n",
       "1228                                                                              Happy Saturday here's 9 puppers on a bench. 99/90 good work everybody https://t.co/mpvaVxKmc1   \n",
       "1254                                                               Here's a brigade of puppers. All look very prepared for whatever happens next. 80/80 https://t.co/0eb7R1Om12   \n",
       "1274                         From left to right:\\nCletus, Jerome, Alejandro, Burp, &amp; Titson\\nNone know where camera is. 45/50 would hug all at once https://t.co/sedre1ivTK   \n",
       "1351                                                                                         Here is a whole flock of puppers.  60/50 I'll take the lot https://t.co/9dpcw6MdWa   \n",
       "1433                                                                               Happy Wednesday here's a bucket of pups. 44/40 would pet all at once https://t.co/HppvrYuamZ   \n",
       "1598                                   Yes I do realize a rating of 4/20 would've been fitting. However, it would be unjust to give these cooperative pups that low of a rating   \n",
       "1634                                             Two sneaky puppers were not initially seen, moving the rating to 143/130. Please forgive us. Thank you https://t.co/kRK51Y5ac3   \n",
       "1635                               Someone help the girl is being mugged. Several are distracting her while two steal her shoes. Clever puppers 121/110 https://t.co/1zfnTJLt55   \n",
       "1662                                This is Darrel. He just robbed a 7/11 and is in a high speed police chase. Was just spotted by the helicopter 10/10 https://t.co/7EsP8LmSp5   \n",
       "1663                                I'm aware that I could've said 20/16, but here at WeRateDogs we are very professional. An inconsistent rating scale is simply irresponsible   \n",
       "1779                                                                                                     IT'S PUPPERGEDDON. Total of 144/120 ...I think https://t.co/ZanVtAtvIq   \n",
       "1843                                                                Here we have an entire platoon of puppers. Total score: 88/80 would pet all at once https://t.co/y93p6FLvVw   \n",
       "2335                                   This is an Albanian 3 1/2 legged  Episcopalian. Loves well-polished hardwood flooring. Penis on the collar. 9/10 https://t.co/d9NcXFKwLv   \n",
       "\n",
       "      rating_denominator  \n",
       "313                    0  \n",
       "342                   15  \n",
       "433                   70  \n",
       "516                    7  \n",
       "784                   11  \n",
       "902                  150  \n",
       "1068                  11  \n",
       "1120                 170  \n",
       "1165                  20  \n",
       "1202                  50  \n",
       "1228                  90  \n",
       "1254                  80  \n",
       "1274                  50  \n",
       "1351                  50  \n",
       "1433                  40  \n",
       "1598                  20  \n",
       "1634                 130  \n",
       "1635                 110  \n",
       "1662                  11  \n",
       "1663                  16  \n",
       "1779                 120  \n",
       "1843                  80  \n",
       "2335                   2  "
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongDenSlice = archive_df_clean [ archive_df_clean['rating_denominator'] != 10]\n",
    "print ('Number of rows with wrong denominator: {}'.format(wrongDenSlice.shape[0]))\n",
    "wrongDenSlice.loc[:,['text','rating_denominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things on the above\n",
    "- When 9/11 is present it doesn't actually refer to a score. There is always a second rating\n",
    "- 24/7 does not repesent a score either...\n",
    "- When there are more than one / it's a date\n",
    "- For the rest of hits, the most sensible will be to apply the following to the numerator. Numerator/Denominator*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTwoRatings = r'(\\d+)\\/(\\d+).*(\\d+)\\/(\\d+)'\n",
    "regexDates = r'(\\d+)\\/(\\d+)\\/(\\d+)'\n",
    "regex24_7 = r'(24\\/7)'\n",
    "tworatings = wrongDenSlice [ wrongDenSlice.text.str.extract(regexTwoRatings)[3].notnull() ]\n",
    "num,den = tworatings.text.str.extract(regexTwoRatings)[2],tworatings.text.str.extract(regexTwoRatings)[3]\n",
    "archive_df_clean.loc[num.index,'rating_numerator'] = num.values\n",
    "archive_df_clean.loc[den.index,'rating_numerator'] = den.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this one as there is no score\n",
    "dates = wrongDenSlice[wrongDenSlice.text.str.extract(regexDates).notnull()[0].values]\n",
    "archive_df_clean.drop(dates.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "data24_7 = wrongDenSlice[ wrongDenSlice.text.str.extract(regex24_7).notnull()[0].values]\n",
    "archive_df_clean.drop(data24_7.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "archive_df_clean.drop( archive_df_clean [ archive_df_clean['rating_denominator'] == 0].index, inplace=True)\n",
    "numeratorToChange = archive_df_clean [ archive_df_clean['rating_denominator'] != 10].index\n",
    "archive_df_clean.loc[numeratorToChange,'rating_numerator'] = (archive_df_clean.loc[numeratorToChange,'rating_numerator'].astype(int).values/archive_df_clean.loc[numeratorToChange,'rating_denominator'].astype(int).values*10).astype(int)\n",
    "archive_df_clean.loc[numeratorToChange,'rating_denominator'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check that, there is no denominator different to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweeted_status_user_id</th>\n",
       "      <th>retweeted_status_timestamp</th>\n",
       "      <th>expanded_urls</th>\n",
       "      <th>rating_numerator</th>\n",
       "      <th>rating_denominator</th>\n",
       "      <th>name</th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>in_reply_to_status_id_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tweet_id, in_reply_to_status_id, in_reply_to_user_id, timestamp, source, text, retweeted_status_user_id, retweeted_status_timestamp, expanded_urls, rating_numerator, rating_denominator, name, doggo, floofer, pupper, puppo, retweeted_status_id, in_reply_to_status_id_2]\n",
       "Index: []"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_df_clean [ archive_df_clean['rating_denominator'] != 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the required information has been collected via Tweeter API, then we need to join it with data in archive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df = archive_df.merge(df_tweeter,on='tweet_id',how='left')\n",
    "archive_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
