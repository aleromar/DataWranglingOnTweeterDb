{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Report\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Gather](#gather)\n",
    "- [Assess](#assess)\n",
    "- [Clean](#clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "The main purpose of this report is to walk through the steps taken to gather, assess and clean the proposed dataset. All these three steps are also known as the wrangling phase and the idea is that this process will help make the analysis phase far less troublesome and more straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Gather'></a>\n",
    "## Gather\n",
    "\n",
    "Data was scattered across three main sources. The way to collect data from those three sources was different in each case and so data collected from each was kept on a separate pandas dataframe. At the end of the gathering process the following dataframes were available\n",
    "\n",
    "- archive_df: This contains data present in twitter_archive_enhanced.csv. This file was downloaded manually and imported using pandas read_csv method. It features tweets sent by WeRateDogs tweeter account over a period of time. For each tweet it includes information such as numerator rating, denominator rating and type of dog to name a few.\n",
    "\n",
    "- image_df: This contains predictions of whether main pictures in each tweet corresponds to a dog, and if so which type. The file containing this information was not present in the same folder as the python script and so it was downloaded programatically. This information can be used jointly with archive_df to remove all those entries that are not dogs (according to the neural network used for predictions)\n",
    "\n",
    "- tweeter_df: We were advised that some of the information contained in archive_df was missing. In this case it was retweet count and favourite count for each of the entries in archive_df. This information was not present in any file, so we had to request these missing pieces from tweeter API.\n",
    "\n",
    "With all these dataframes ready and loaded we were ready to proceed to the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## Assess\n",
    "\n",
    "Alternatively using both visual and programatic inspection I was able to spot a few issues in data available. Some affect data quality and others affect dataframe tidiness. Let's summarize the findings below in bullet points, each point refers to an issue found when assessing the three dataframes available to us.\n",
    "\n",
    "- A column in *archive_df* indicated whether a particular tweet was a retweet. If a particular tweet is a retweet, then there is a risk the same dog is rated twice (in the original tweet and the retweet). Therefore we need to check whether there instances where for a given retweet the original is present. It was found that there were a few of them in this situation and so they need to be addressed.\n",
    "\n",
    "- Some of the columns were in the wrong format. Perhaps the most obvious was timestamps that were strings in the orginal dataframe. Also IDs should be integers and not floating point type. This needs addressing too.\n",
    "\n",
    "- Similarly to the first point, it may well be that tweets that were a reply to another tweet were duplicating information. For example when replying to their own account tweets. This in turn means that again the same dog is rated twice. It was found that there were a few of these and need addressing too.\n",
    "\n",
    "- The column expanded urls in *archive_df* dataframe contained missing information. Data for this needs to be gathered and so stressing the importance of iterative wrangling.\n",
    "\n",
    "- It was observed that some of the pet names in the column *names* in *archive_df* were not actually names but rather normal words. It was also seen that the mistakes shared the same trait. They all had the first letter in smaller case. This needs to be taken care of as well in the cleaning phase.\n",
    "\n",
    "- According to the instructions that we were given, denominators were supposed to be always 10. It was observed that this wasn't the case in the dataframe that was available to us. Therefore something needed to be done.\n",
    "\n",
    "- Now moving on to image_df if was observed that predictions were not always dogs... Or something dogs were not the first prediction from the neural network. This, as I could observe when looking at the picture for myself was in part due to the fact that a different object was occupying a more relevant position in the picture or was bigger. It makes no sense to have tweets that are not dogs. This needs addressing.\n",
    "\n",
    "- Again back to *archive_df* the column *source* contains odd-looking content. When digging deeper it can be seen that this column contains values from only four possible categories. However values are wrapped in an xml tag format that needs cleaning.\n",
    "\n",
    "The 8 issues describe above can be regarded as quality issues. Also two other issues were found that deal more with data structure or tidiness\n",
    "\n",
    "- There are four columns in *archive_df*, namely *doggo*, *floofer*, *pupper*, *puppo*, that are actually the same variable, however they are distributing occupying four different columns. This is a flaw in the structure of the dataframe and needs addressing.\n",
    "\n",
    "- Also tweeter_df and archive_df can be regarded as part of the same observational unit. Therefore these two distinct subframes need be joint. This will mean that for the analysis phase there will be only two dataframes to look at.\n",
    "\n",
    "Once a number of issues have been found, let's proceed to fix them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "## Clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 -> Retweets\n",
    "The first issue highlighted above was resolved using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df_clean = archive_df.copy()\n",
    "archive_df_clean.drop(['retweeted_status_id'],axis=1,inplace=True)\n",
    "archive_df_clean.rename(columns={'retweeted_status_id_2':'retweeted_status_id'},inplace=True)\n",
    "retweets = archive_df_clean[archive_df_clean.retweeted_status_id.notnull()]\n",
    "duplicated = np.isin(retweets['retweeted_status_id'].astype('int64').values,archive_df_clean.tweet_id.values)\n",
    "retweets = retweets[duplicated]\n",
    "archive_df_clean.drop(retweets.index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to find those retweets whose original is already part of the dataframe. In those cases keep the original and **drop** the retweet. Testing that this issue was corrected can be easily done by checking the number of entries in the dataframe (it should decrease by the same number of duplicate information in retweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 -> Timestamp columns to datetime\n",
    "Second issue is to cast columns to the appropriate type. For example columns representing timestamps or IDs as ints or string (but not floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df_clean['timestamp'] = pd.to_datetime(archive_df_clean.timestamp)\n",
    "archive_df_clean['retweeted_status_timestamp'] = pd.to_datetime(archive_df_clean.retweeted_status_timestamp)\n",
    "archive_df_clean.in_reply_to_user_id = archive_df_clean['in_reply_to_user_id'].astype(str).str.extract('(\\d+)')\n",
    "archive_df_clean.retweeted_status_user_id = archive_df_clean['retweeted_status_user_id'].astype(str).str.extract('(\\d+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 -> Replies to tweets\n",
    "Third issue is very similar to the first one. Need removing all those replies for which an original is already present in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweet id for which the tweet was a reply\n",
    "num_rows_before_cleaning_replies = archive_df_clean.shape[0]\n",
    "archive_df_clean.drop(['in_reply_to_status_id'],axis=1,inplace=True)\n",
    "archive_df_clean.rename(columns={'in_reply_to_status_id_2':'in_reply_to_status_id'},inplace=True)\n",
    "replies = archive_df_clean[archive_df_clean.in_reply_to_status_id.notnull()]\n",
    "duplicated = np.isin(replies['in_reply_to_status_id'].astype('int64').values,archive_df_clean.tweet_id.values)\n",
    "replies = replies[duplicated]\n",
    "archive_df_clean.drop(replies.index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was tested using the same methodology as for issue \\#1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 -> Missing information in expanded url column\n",
    "Json data downloaded from tweeter was used to collect all the missing information for this column. The code below resolved the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweet id for which the tweet was a reply\n",
    "listTweetsNoUrl = archive_df_clean[archive_df_clean['expanded_urls'].isnull()].tweet_id.values\n",
    "\n",
    "with open('tweet_json.txt','r',encoding='utf-8') as fjsonRead:\n",
    "    for line in fjsonRead:\n",
    "        json_dict = json.loads(line)\n",
    "        if json_dict['id'] in listTweetsNoUrl:\n",
    "            archive_df_clean.loc[archive_df_clean[archive_df_clean['tweet_id']==json_dict['id']].index,'expanded_urls'] = json_dict['user']['entities']['url']['urls'][0]['expanded_url']            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 -> Not all names are pet names\n",
    "This issue was probably the most tricky. First we listed all those entries that contained a wrong name type. This allowed us to look for patterns that were perhaps not properly used originally. Then and using regular expressions some of the names were correctly extracted and some were not present. For all those that were not present it was decided to use *None*. Then *None* was removed and use np.NaN so it was clear that for some tweets there was no name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 -> Denominators different to 10\n",
    "Again this one was not very straight forward. In general I find that those that deal with regular expressions are probably the most challenging issues to clean. This time, the dataframe was filtered extracting all those entries that did not contain a denominator equals to 10. Then text linked to those entries was analysed visually to check for patterns. It could be seen that some of the wrong entries had two ratios and in fact the second ratio was the correct one. Also, some others it was found that it was not a ratio what was present in the text, but rather a date or 24/7 indicating availability, etc. These were fixed however there were some that we couldn't find a proper reason and so it was decided to scale the numerator to fix the denominator to 10. Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongDenSlice = archive_df_clean [ archive_df_clean['rating_denominator'] != 10]\n",
    "print ('Number of rows with wrong denominator: {}'.format(wrongDenSlice.shape[0]))\n",
    "wrongDenSlice.loc[:,['text','rating_denominator']]\n",
    "regexTwoRatings = r'(\\d+)\\/(\\d+).*(\\d+)\\/(\\d+)'\n",
    "regexDates = r'(\\d+)\\/(\\d+)\\/(\\d+)'\n",
    "regex24_7 = r'(24\\/7)'\n",
    "tworatings = wrongDenSlice [ wrongDenSlice.text.str.extract(regexTwoRatings)[3].notnull() ]\n",
    "num,den = tworatings.text.str.extract(regexTwoRatings)[2],tworatings.text.str.extract(regexTwoRatings)[3]\n",
    "archive_df_clean.loc[num.index,'rating_numerator'] = num.values\n",
    "archive_df_clean.loc[den.index,'rating_numerator'] = den.values\n",
    "# Remove this one as there is no score\n",
    "dates = wrongDenSlice[wrongDenSlice.text.str.extract(regexDates).notnull()[0].values]\n",
    "archive_df_clean.drop(dates.index, inplace=True)\n",
    "data24_7 = wrongDenSlice[ wrongDenSlice.text.str.extract(regex24_7).notnull()[0].values]\n",
    "archive_df_clean.drop(data24_7.index, inplace=True)\n",
    "archive_df_clean.drop( archive_df_clean [ archive_df_clean['rating_denominator'] == 0].index, inplace=True)\n",
    "numeratorToChange = archive_df_clean [ archive_df_clean['rating_denominator'] != 10].index\n",
    "archive_df_clean.loc[numeratorToChange,'rating_numerator'] = (archive_df_clean.loc[numeratorToChange,'rating_numerator'].astype(int).values/archive_df_clean.loc[numeratorToChange,'rating_denominator'].astype(int).values*10).astype(int)\n",
    "archive_df_clean.loc[numeratorToChange,'rating_denominator'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing that code work was in this case very easy. Using describe() on denominator column gives clear results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 -> Predictions are not always dogs\n",
    "So does it make sense to have tweets that don't contain dogs in the image? Probably not. However some of the tweets did not contain dogs. Also some of the predictions were dogs but were assigned lower probability than the non-dog object. So this needs cleaning as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shiftColumns(df,indexToUse,lleft,lright):\n",
    "    df.loc[indexToUse,lleft[0]] = df.loc[indexToUse,lright[0]]\n",
    "    df.loc[indexToUse,lleft[1]] = df.loc[indexToUse,lright[1]]\n",
    "    df.loc[indexToUse,lleft[2]] = df.loc[indexToUse,lright[2]]\n",
    "    df.loc[indexToUse,lright] = np.NaN\n",
    "    \n",
    "image_df_clean = image_df.copy()\n",
    "# Let's first remove all those rows that do not contain any prediction of a dog\n",
    "image_df_clean.drop(image_df[ (image_df_clean.p1_dog == False) & (image_df_clean.p2_dog == False) & (image_df_clean.p3_dog == False) ].index, inplace=True)\n",
    "# Let's now make NaN all those p3_dog == False\n",
    "image_df_clean.loc[image_df_clean[ (image_df_clean.p3_dog == False) ].index, ['p3','p3_conf','p3_dog']] = np.NaN\n",
    "# Let's now make NaN all those p2_dog == False\n",
    "image2Index = image_df_clean[ (image_df_clean.p2_dog == False) ].index\n",
    "shiftColumns(image_df_clean,image2Index,['p2','p2_conf','p2_dog'],['p3','p3_conf','p3_dog'])\n",
    "# Let's now make NaN all those p1_dog == False\n",
    "image1Index = image_df_clean[ (image_df_clean.p1_dog == False) ].index\n",
    "shiftColumns(image_df_clean,image1Index,['p1','p1_conf','p1_dog'],['p2','p2_conf','p2_dog'])\n",
    "shiftColumns(image_df_clean,image1Index,['p2','p2_conf','p2_dog'],['p3','p3_conf','p3_dog'])\n",
    "# Changes booleans to the appropriate type\n",
    "image_df_clean.p3_dog = image_df_clean.p3_dog.map(lambda x: True if x in [True, 1.0] else np.NaN)\n",
    "image_df_clean.p2_dog = image_df_clean.p2_dog.map(lambda x: True if x in [True, 1.0] else np.NaN)\n",
    "image_df_clean.p1_dog = image_df_clean.p1_dog.map(lambda x: True if x in [True, 1.0] else np.NaN)\n",
    "image_df_clean.loc[1034]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 -> Properly categorise *source* column\n",
    "Source column contains categories that are wrapped with xml tags. Remove them to make them clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df_clean.source = archive_df_clean.source.str.extract('(\">)(.*)(<\\/a)')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it using value counts is sufficient to observe that we got the desired behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9 -> Dog type is scattered across four columns\n",
    "This is the first of the structural problems described above. The idea is to have only one column representing one variable. In this case the variable is dog type. The following code has been used to achieve desired results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df_clean.replace({'doggo':1,'floofer':1,'pupper':1,'puppo':1},inplace=True)\n",
    "dogCat = pd.melt(archive_df_clean,id_vars=['tweet_id'],value_vars=['doggo', 'floofer', 'pupper', 'puppo'],var_name='category',value_name='valueToRemove')\n",
    "dogCat.valueToRemove = dogCat.valueToRemove.replace('None', np.NaN)\n",
    "dogCat = dogCat.dropna()\n",
    "archive_df_clean = archive_df_clean.merge(dogCat,how='left',on='tweet_id')\n",
    "archive_df_clean.drop(['doggo', 'floofer', 'pupper', 'puppo','valueToRemove'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 -> Two dataframes correspond to the same observational unit\n",
    "The idea here is to merge two dataframes that contain variables that are actually part of the same observational unit. A join operation, in SQL language, needs to be done for this to happen. Let's look at the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df_clean = archive_df_clean.merge(tweeter_df,on='tweet_id',how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really easy to test by looking at the first few entries of the resulting dataframe and checking that all variables are there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
